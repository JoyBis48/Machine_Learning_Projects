{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Traverse the directory structure\n",
    "path = r'C:\\Users\\User\\Machine_Learning_Projects\\Depression_Analysis\\2017'\n",
    "\n",
    "train_folder = os.path.join(path, 'train')\n",
    "test_folder = os.path.join(path, 'test')\n",
    "\n",
    "# Step 2-4: Read XML files and load data into an appropriate format\n",
    "def process_xml_files(folder_path, label):\n",
    "    data = []\n",
    "\n",
    "    for chunk_folder in os.listdir(folder_path):\n",
    "        chunk_folder_path = os.path.join(folder_path, chunk_folder)\n",
    "        if not os.path.isdir(chunk_folder_path):\n",
    "            continue\n",
    "\n",
    "        for xml_file in os.listdir(chunk_folder_path):\n",
    "            xml_file_path = os.path.join(chunk_folder_path, xml_file)\n",
    "            if not xml_file.endswith('.xml'):\n",
    "                continue\n",
    "\n",
    "            # Step 3: Extract data from XML files\n",
    "            tree = et.parse(xml_file_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract the relevant data from the XML structure\n",
    "            # Modify this part based on the XML structure of your dataset\n",
    "            for writing in root.findall('WRITING'):\n",
    "                text = writing.find('TEXT').text.strip()\n",
    "\n",
    "\n",
    "                # Step 4: Load the data into an appropriate format\n",
    "                data.append({'text': text, 'label': label})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Process the train dataset\n",
    "train_data_depression = process_xml_files(os.path.join(train_folder, 'positive_examples_anonymous_chunks'), label='depression')\n",
    "train_data_non_depression = process_xml_files(os.path.join(train_folder, 'negative_examples_anonymous_chunks'), label='non-depression')\n",
    "\n",
    "# Combining the train datasets into a single DataFrame\n",
    "train_df = pd.DataFrame(train_data_depression + train_data_non_depression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "train_df.text = train_df.text.apply(gensim.utils.simple_preprocess)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=5,\n",
    "    min_count = 2,\n",
    "    workers=4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.build_vocab(train_df.text, progress_per=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train(train_df.text, total_examples=model.corpus_count, epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('exp.model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('exp.model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].map({\n",
    "    'non-depression':0,\n",
    "    'depression':1\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.label.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def buildvector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "vectors = np.concatenate([buildvector(x, model.wv.vector_size) for x in train_df.text])\n",
    "vectors = scale(vectors)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vectors, train_df.label, test_size=0.2, random_state=348)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "print(\"Logistic Regression accuracy:\", accuracy)\n",
    "print(\"Logistic Regression recall score:\", recall)\n",
    "print(\"Logistic Regression precison score:\", precision)\n",
    "print(\"Logistic Regression f1 score :\", f1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standardize the feature vectors\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform oversampling using SMOTE\n",
    "oversampler = SMOTE(random_state=342)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train_scaled, Y_train)\n",
    "\n",
    "# Perform undersampling using RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(random_state=342)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Train logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(Y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluation of the Model using ROC Curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Predicting probabilities on the test set\n",
    "probs = logreg.predict_proba(X_test)\n",
    "probs_positive = probs[:, 1]  # Considering only positive class probabilities\n",
    "\n",
    "# Calculating the false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, probs_positive)\n",
    "\n",
    "# Calculate the Area Under the ROC Curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'r--')  # Plotting the random classifier\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluating the Peformance using Confusion Matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Creating a heatmap for the confusion matrix\n",
    "labels = np.unique(Y_test)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix Plot')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## UNDER CONSTRUCTION ##\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidfmodel = TfidfVectorizer()\n",
    "# inputs = tfidfmodel.fit_transform(df['text_without_stopwords'])\n",
    "# print(inputs)\n",
    "# tfidfmodel.vocabulary_\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(inputs, df.label, test_size=0.2, random_state=48)\n",
    "# X_train.shape\n",
    "# Y_train.shape\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr_model = LogisticRegression()\n",
    "# lr_model.fit(X_train, Y_train)\n",
    "# from sklearn.metrics import accuracy_score, recall_score\n",
    "# predictions = lr_model.predict(X_test)\n",
    "# accuracy = accuracy_score(Y_test, predictions)\n",
    "# recall = recall_score(Y_test, predictions)\n",
    "# print(accuracy)\n",
    "# print(recall)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Converting text data to Word2Vec embeddings\n",
    "# X_vectors = []\n",
    "# for items in df.lemmatized_text:\n",
    "#     vectors = [word2vec_model.wv[word] for word in items if word in word2vec_model.wv]\n",
    "#     if vectors:\n",
    "#         X_vectors.append(np.mean(vectors, axis=0)) # This will result in a single vector representation for the whole sentence. Each sentence will now be represented as a vector of size 100. (vector size =100 by default)\n",
    "# #Splitting the dataset\n",
    "# X_array = np.array(X_vectors)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_array, df.label, test_size=0.2, random_state=48)\n",
    "# X_train.shape\n",
    "# Y_train.shape\n",
    "# X_test.shape\n",
    "# Y_test.shape\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "#\n",
    "# # Create a Logistic Regression model\n",
    "# lr_model = LogisticRegression()\n",
    "#\n",
    "# # Train the model\n",
    "# lr_model.fit(X_train, Y_train)\n",
    "#\n",
    "# # Make predictions on the test set\n",
    "# y_pred = lr_model.predict(X_test)\n",
    "#\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(Y_test, y_pred)\n",
    "# recall = recall_score(Y_test, y_pred)\n",
    "# precision = precision_score(Y_test, y_pred)\n",
    "# f1 = f1_score(Y_test, y_pred)\n",
    "# print(\"Logistic Regression accuracy:\", accuracy)\n",
    "# print(\"Logistic Regression recall score:\", recall)\n",
    "# print(\"Logistic Regression precison score:\", precision)\n",
    "# print(\"Logistic Regression f1 score :\", f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
